input-dir input map_zero.txt
[32m2021-10-07 16:27:43.626[39m | [1mINFO[22m | [90m/home/av/Desktop/Multi-Agent-Allocation-with-Generative-Network/environment/level_utils.py:75[39m | Tokens in level ['-', 'W', 'X']
Library size increased: 1
MODE: train
WHÄ°LE WHÄ°LE WHÄ°LE WHÄ°LE WHÄ°LE WHÄ°LE WHÄ°LE WHÄ°LE WHÄ°LE WHÄ°LE WHÄ°LE WHÄ°LE WHÄ°LE WHÄ°LE WHÄ°LE WHÄ°LE WHÄ°LE WHÄ°LE WHÄ°LE WHÄ°LE WHÄ°LE WHÄ°LE WHÄ°LE WHÄ°LE WHÄ°LE WHÄ°LE WHÄ°LE WHÄ°LE WHÄ°LE WHÄ°LE
0
agent map for loop [[1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1.]
 [1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1.]
 [0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]
 [1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1.]
 [0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1.]
 [1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0.]
 [0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]
 [1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1.]
 [0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0.]
 [1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0.]
 [1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0.]
 [0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1.]
 [1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0.]
 [0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1.]
 [0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0.]
 [1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0.]]
  0%|                                                                             | 0/15 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "main.py", line 198, in <module>
    main()
  File "main.py", line 93, in main
    generated_map = g.train(env, sample_map, classifier, opt)
  File "/home/av/Desktop/Multi-Agent-Allocation-with-Generative-Network/train.py", line 110, in train
    reward = env.reset_and_step(ds_map, obstacle_map, prize_map, agent_map, map_lim, obs_y_list, obs_x_list, i+1)
  File "/home/av/Desktop/Multi-Agent-Allocation-with-Generative-Network/env_funcs.py", line 23, in reset_and_step
    episode_reward, _, _ = self.env.step(n_agents)
ValueError: not enough values to unpack (expected 3, got 2)