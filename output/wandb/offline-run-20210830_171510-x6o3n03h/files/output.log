[32m2021-08-30 17:15:12.407[39m | [1mINFO[22m | [90m/home/regen/Desktop/Multi-Agent-Allocation-with-Generative-Network/environment/level_utils.py:75[39m | Tokens in level ['-', 'O', 'W', 'X']
Level_GeneratorConcatSkip2CleanAdd(
  (head): ConvBlock(
    (conv): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1))
    (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (body): Sequential(
    (block1): ConvBlock(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
      (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)
    )
  )
  (tail): Sequential(
    (0): Conv2d(64, 4, kernel_size=(3, 3), stride=(1, 1))
  )
)
Level_WDiscriminator(
  (head): ConvBlock(
    (conv): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1))
    (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (body): Sequential(
    (block1): ConvBlock(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
      (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)
    )
  )
  (tail): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1))
)
[32m2021-08-30 17:15:14.514[39m | [1mINFO[22m | [90m/home/regen/Desktop/Multi-Agent-Allocation-with-Generative-Network/train_single_scale.py:63[39m | Training at scale 0
self.n_agents:  1
  0%|                                                                                                                                                                                                                        | 0/100 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "main.py", line 83, in <module>
    main()
  File "main.py", line 58, in main
    generators, noise_maps, reals, noise_amplitudes = train(real, opt)
  File "/home/regen/Desktop/Multi-Agent-Allocation-with-Generative-Network/train.py", line 72, in train
    input_from_prev_scale, noise_amplitudes, opt)
  File "/home/regen/Desktop/Multi-Agent-Allocation-with-Generative-Network/train_single_scale.py", line 168, in train_single_scale
    agent_mean_reward = RL.train(coded_fake_map, current_scale, epoch)
  File "/home/regen/Desktop/Multi-Agent-Allocation-with-Generative-Network/rl_agent.py", line 82, in train
    episode_reward, done, agent_next_obs = self.env.step(n_agents)
  File "/home/regen/Desktop/Multi-Agent-Allocation-with-Generative-Network/point_mass_formation.py", line 81, in step
    self.agents_action_list[agent_ind], pos_list, feasible = self.create_trajectory(agent_ind)
  File "/home/regen/Desktop/Multi-Agent-Allocation-with-Generative-Network/point_mass_formation.py", line 217, in create_trajectory
    feasible, pos_list, action_list = self.dstar.run(start, end)
  File "/home/regen/Desktop/Multi-Agent-Allocation-with-Generative-Network/dstar.py", line 197, in run
    if tmp.parent.state == "O":
AttributeError: 'NoneType' object has no attribute 'state'